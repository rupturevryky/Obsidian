## Введение
**Uscrapper** — это инструмент командной строки, предназначенный для поиска и извлечения личных данных с веб-сайтов. Он обходит страницы и собирает с помощью регулярных выражений такие данные, как адреса электронной почты, ссылки на социальные сети, имена людей, номера телефонов и юзернеймы. Имеет несколько модулей для обхода защиты от парсинга и после завершения работы создает отчет со всеми найденными данными. Uscrapper можно использовать как легковесную альтернативу SpiderFoot на этапе сбора информации.

---

## Установка и использование​

Клонируем репозиторий и запускаем файл `install`:
```
git clone https://github.com/z0m31en7/Uscrapper.git
cd Uscrapper/install/
chmod +x ./install.sh && ./install.sh
```
После чего мы можем приступать к использованию, для этого нам понадобится домен интересующего вас сайта:
```
python Uscrapper-vanta.py -u site
```

В целом инструмент собирает любые персональные данные которые представлены на целевом сайте. Можно добавить параметры для генерного отчёта, поиска по ключевым словам или путь к файлу содержащему ключевые слова. Всё это можно получить, применив следующие аргументы:  
- `-u URL`, `–url URL` (URL веб-сайта)  
- `-O`, `–generate-report` (Сгенерировать отчет)  
- `-ns`, `–nonstrict` (Отобразить имена пользователей с неточным совпадением (может показать неточные результаты))  
- `-c CRAWL`, `–crawl` (CRAWL) укажите максимальное количество ссылок для обхода и извлечения в рамках одного диапазона  
- `-t THREADS`, `–threads THREADS` (Количество потоков для использования при обходе (по умолчанию=4))  
- `-k KEYWORDS [KEYWORDS ...]`, `–keywords KEYWORDS [KEYWORDS ...]` (Ключевые слова для поиска (в виде аргументов, разделенных пробелом))  
- `-f FILE`,` –file FILE` (Путь к текстовому файлу, содержащему ключевые слова)